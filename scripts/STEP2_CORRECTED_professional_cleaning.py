"""
CORRECTED PROFESSIONAL DATA CLEANING WITH PROPER NAME MAPPING
==============================================================
This script handles:
1. State/UT name variations and typos
2. Historical names (Orissa ‚Üí Odisha, Pondicherry ‚Üí Puducherry)
3. Naming format differences (& vs and, extra spaces)
4. Proper standardization before validation
"""

import pandas as pd
import numpy as np
import glob
import os
from datetime import datetime
import re

# ============================================
# CONFIGURATION
# ============================================
BASE_PATH = r"E:\Aadhar UIDAI"
PROJECT_PATH = os.path.join(BASE_PATH, "PROJECT")
OUTPUT_FOLDER = os.path.join(PROJECT_PATH, "data", "processed")
RESULTS_FOLDER = os.path.join(PROJECT_PATH, "results")

os.makedirs(OUTPUT_FOLDER, exist_ok=True)
os.makedirs(RESULTS_FOLDER, exist_ok=True)

ENROLMENT_FOLDER = os.path.join(BASE_PATH, "api_data_aadhar_enrolment", "api_data_aadhar_enrolment")
BIOMETRIC_FOLDER = os.path.join(BASE_PATH, "api_data_aadhar_biometric", "api_data_aadhar_biometric")
DEMOGRAPHIC_FOLDER = os.path.join(BASE_PATH, "api_data_aadhar_demographic", "api_data_aadhar_demographic")

# ============================================
# VALID STATES & UTs (FINAL STANDARDIZED NAMES)
# ============================================
VALID_STATES_UTS = {
    # 28 States
    'andhra pradesh', 'arunachal pradesh', 'assam', 'bihar', 'chhattisgarh',
    'goa', 'gujarat', 'haryana', 'himachal pradesh', 'jharkhand',
    'karnataka', 'kerala', 'madhya pradesh', 'maharashtra', 'manipur',
    'meghalaya', 'mizoram', 'nagaland', 'odisha', 'punjab',
    'rajasthan', 'sikkim', 'tamil nadu', 'telangana', 'tripura',
    'uttar pradesh', 'uttarakhand', 'west bengal',
    
    # 8 Union Territories
    'andaman and nicobar islands', 'chandigarh', 
    'dadra and nagar haveli and daman and diu',
    'delhi', 'jammu and kashmir', 'ladakh', 'lakshadweep', 'puducherry'
}

# ============================================
# STATE NAME MAPPING & CORRECTIONS
# ============================================
STATE_NAME_CORRECTIONS = {
    # Historical names
    'orissa': 'odisha',
    'pondicherry': 'puducherry',
    'uttaranchal': 'uttarakhand',
    
    # Typos and variations
    'west bangal': 'west bengal',
    'westbengal': 'west bengal',
    'west  bengal': 'west bengal',  # extra space
    
    # Union Territory variations with &
    'andaman & nicobar islands': 'andaman and nicobar islands',
    'dadra & nagar haveli': 'dadra and nagar haveli and daman and diu',
    'dadra and nagar haveli': 'dadra and nagar haveli and daman and diu',
    'daman and diu': 'dadra and nagar haveli and daman and diu',
    'daman & diu': 'dadra and nagar haveli and daman and diu',
    'jammu & kashmir': 'jammu and kashmir',
    
    # The/extra words
    'the dadra and nagar haveli and daman and diu': 'dadra and nagar haveli and daman and diu',
    
    # NCT variations
    'nct of delhi': 'delhi',
    'new delhi': 'delhi',
    
    # Other variations
    'pondichery': 'puducherry',
    'puduchery': 'puducherry',
}

print("=" * 120)
print("CORRECTED PROFESSIONAL DATA CLEANING WITH PROPER NAME MAPPING")
print("=" * 120)
print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"\n‚úì Reference: 28 States + 8 Union Territories = 36 valid regions")
print(f"‚úì Loaded {len(STATE_NAME_CORRECTIONS)} name corrections/mappings")
print("=" * 120)

# ============================================
# FUNCTION: CLEAN AND STANDARDIZE STATE NAMES
# ============================================
def clean_state_name(state_name):
    """
    Clean and standardize state/UT names
    Handles: typos, historical names, variations, extra spaces
    """
    if pd.isna(state_name):
        return None
    
    # Convert to lowercase and strip
    state = str(state_name).lower().strip()
    
    # Remove extra spaces (multiple spaces to single space)
    state = re.sub(r'\s+', ' ', state)
    
    # Apply corrections mapping
    if state in STATE_NAME_CORRECTIONS:
        state = STATE_NAME_CORRECTIONS[state]
    
    return state

# ============================================
# CLEANING FUNCTION
# ============================================
def professional_data_cleaning(folder_path, dataset_name, columns_to_check):
    """
    Professional data cleaning with proper name standardization
    """
    
    print(f"\n{'=' * 120}")
    print(f"PROCESSING: {dataset_name.upper()} DATASET")
    print(f"{'=' * 120}")
    
    # ========================================
    # STEP 1: DATA LOADING
    # ========================================
    print(f"\n{'‚îÄ' * 120}")
    print("STEP 1: DATA LOADING")
    print(f"{'‚îÄ' * 120}")
    
    csv_files = glob.glob(os.path.join(folder_path, "*.csv"))
    print(f"\n‚úì Found {len(csv_files)} CSV files")
    
    dataframes = []
    total_rows_before = 0
    
    for i, file in enumerate(csv_files, 1):
        file_size = os.path.getsize(file) / (1024 * 1024)
        df_temp = pd.read_csv(file)
        rows = len(df_temp)
        total_rows_before += rows
        dataframes.append(df_temp)
        print(f"  [{i}/{len(csv_files)}] {os.path.basename(file):50s} | {rows:>10,} rows | {file_size:>8.2f} MB")
    
    df = pd.concat(dataframes, ignore_index=True)
    print(f"\n‚úì Combined: {len(df):,} total rows")
    
    quality_report = {
        'initial_rows': len(df),
        'issues_found': [],
        'rows_removed': {},
        'corrections_made': {}
    }
    
    # ========================================
    # STEP 2: CLEAN AND STANDARDIZE STATE NAMES
    # ========================================
    print(f"\n{'‚îÄ' * 120}")
    print("STEP 2: STATE NAME CLEANING & STANDARDIZATION")
    print(f"{'‚îÄ' * 120}")
    
    # Show unique states before cleaning
    unique_before = df['state'].unique()
    print(f"\nüìä Unique state values BEFORE cleaning: {len(unique_before)}")
    
    # Apply cleaning function
    df['state_original'] = df['state']  # Keep original for reference
    df['state'] = df['state'].apply(clean_state_name)
    
    # Show unique states after cleaning
    unique_after = df['state'].unique()
    print(f"üìä Unique state values AFTER cleaning: {len([s for s in unique_after if pd.notna(s)])}")
    
    # Show what corrections were made
    print(f"\nüîß Corrections Applied:")
    corrections_count = {}
    for idx, row in df.iterrows():
        if pd.notna(row['state_original']) and row['state_original'].lower().strip() != row['state']:
            original = row['state_original']
            corrected = row['state']
            key = f"{original} ‚Üí {corrected}"
            corrections_count[key] = corrections_count.get(key, 0) + 1
    
    if corrections_count:
        for correction, count in sorted(corrections_count.items(), key=lambda x: x[1], reverse=True)[:20]:
            print(f"  {correction:70s} : {count:>8,} rows")
        quality_report['corrections_made']['state_corrections'] = len(corrections_count)
    else:
        print("  No corrections needed - all states were already standard")
    
    # ========================================
    # STEP 3: VALIDATE STATE NAMES
    # ========================================
    print(f"\n{'‚îÄ' * 120}")
    print("STEP 3: STATE NAME VALIDATION (After Corrections)")
    print(f"{'‚îÄ' * 120}")
    
    # Find invalid states (after corrections)
    unique_states_cleaned = [s for s in df['state'].unique() if pd.notna(s)]
    print(f"\nüìä Found {len(unique_states_cleaned)} unique cleaned state values")
    
    invalid_states = []
    for state in unique_states_cleaned:
        if state not in VALID_STATES_UTS:
            # Check if it's a number
            if str(state).replace(' ', '').replace('.', '').isdigit():
                invalid_states.append(state)
                print(f"  ‚ö†Ô∏è  INVALID (Numeric): '{state}' - {len(df[df['state'] == state]):,} rows")
            else:
                invalid_states.append(state)
                print(f"  ‚ö†Ô∏è  INVALID (Unknown): '{state}' - {len(df[df['state'] == state]):,} rows")
    
    # Also check for NULL
    null_states = df['state'].isnull().sum()
    if null_states > 0:
        print(f"  ‚ö†Ô∏è  NULL/Missing: {null_states:,} rows")
        invalid_states.append(None)
    
    if invalid_states:
        invalid_count = len(df[df['state'].isin(invalid_states) | df['state'].isnull()])
        print(f"\n‚ö†Ô∏è  REMOVING {invalid_count:,} rows with invalid states")
        quality_report['issues_found'].append(f"Invalid states: {len(invalid_states)}")
        quality_report['rows_removed']['invalid_states'] = invalid_count
        
        # Remove invalid states
        df = df[~(df['state'].isin(invalid_states) | df['state'].isnull())]
        print(f"‚úì Removed rows. Remaining: {len(df):,}")
    else:
        print(f"\n‚úì All states are valid after corrections!")
    
    # Show valid states present
    valid_states_in_data = sorted([s for s in df['state'].unique() if s in VALID_STATES_UTS])
    print(f"\n‚úì Valid states/UTs present in data: {len(valid_states_in_data)}/36")
    
    # Show all valid states
    print(f"\nValid states/UTs in data:")
    for i, state in enumerate(valid_states_in_data, 1):
        count = len(df[df['state'] == state])
        print(f"  {i:2d}. {state:45s} - {count:>10,} rows")
    
    # ========================================
    # STEP 4: STANDARDIZE DISTRICT NAMES
    # ========================================
    print(f"\n{'‚îÄ' * 120}")
    print("STEP 4: DISTRICT NAME STANDARDIZATION")
    print(f"{'‚îÄ' * 120}")
    
    # Clean district names (lowercase, remove extra spaces)
    df['district'] = df['district'].str.lower().str.strip()
    df['district'] = df['district'].apply(lambda x: re.sub(r'\s+', ' ', str(x)) if pd.notna(x) else x)
    
    print(f"\n‚úì Standardized district names")
    print(f"  Unique districts: {df['district'].nunique()}")
    
    # ========================================
    # STEP 5: VALIDATE PINCODE
    # ========================================
    print(f"\n{'‚îÄ' * 120}")
    print("STEP 5: PINCODE VALIDATION")
    print(f"{'‚îÄ' * 120}")
    
    df['pincode'] = df['pincode'].astype(str).str.strip()
    
    # Indian pincodes must be 6 digits
    invalid_pincode_mask = (df['pincode'].str.len() != 6) | (~df['pincode'].str.isdigit())
    invalid_pincode_count = invalid_pincode_mask.sum()
    
    if invalid_pincode_count > 0:
        print(f"\n‚ö†Ô∏è  Found {invalid_pincode_count:,} rows with invalid pincodes")
        sample_invalid = df[invalid_pincode_mask]['pincode'].value_counts().head(5)
        print(f"  Sample invalid pincodes:")
        for pin, count in sample_invalid.items():
            print(f"    '{pin}' - {count:,} rows")
        
        quality_report['issues_found'].append(f"Invalid pincodes: {invalid_pincode_count}")
        quality_report['rows_removed']['invalid_pincodes'] = invalid_pincode_count
        
        df = df[~invalid_pincode_mask]
        print(f"‚úì Removed rows. Remaining: {len(df):,}")
    else:
        print(f"\n‚úì All pincodes are valid (6-digit)")
    
    # ========================================
    # STEP 6: VALIDATE NUMERIC COLUMNS
    # ========================================
    print(f"\n{'‚îÄ' * 120}")
    print("STEP 6: NUMERIC COLUMN VALIDATION")
    print(f"{'‚îÄ' * 120}")
    
    numeric_cols = [c for c in df.columns if c not in ['date', 'state', 'district', 'pincode', 'state_original']]
    
    total_negative = 0
    for col in numeric_cols:
        negative_count = (df[col] < 0).sum()
        if negative_count > 0:
            print(f"\n‚ö†Ô∏è  Found {negative_count:,} negative values in '{col}'")
            total_negative += negative_count
            quality_report['issues_found'].append(f"Negative values in {col}: {negative_count}")
            
            df = df[df[col] >= 0]
    
    if total_negative > 0:
        quality_report['rows_removed']['negative_values'] = total_negative
        print(f"\n‚úì Removed rows with negative values. Remaining: {len(df):,}")
    else:
        print(f"\n‚úì No negative values found")
    
    print(f"\n‚úì Numeric columns statistics:")
    for col in numeric_cols:
        if col in df.columns:
            print(f"  {col:25s} | Min: {df[col].min():>8,.0f} | Max: {df[col].max():>10,.0f} | Mean: {df[col].mean():>8,.2f}")
    
    # ========================================
    # STEP 7: DATE VALIDATION & PARSING
    # ========================================
    print(f"\n{'‚îÄ' * 120}")
    print("STEP 7: DATE VALIDATION & PARSING")
    print(f"{'‚îÄ' * 120}")
    
    try:
        df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y', errors='coerce')
        invalid_dates = df['date'].isnull().sum()
        
        if invalid_dates > 0:
            print(f"\n‚ö†Ô∏è  Found {invalid_dates:,} invalid dates")
            quality_report['issues_found'].append(f"Invalid dates: {invalid_dates}")
            quality_report['rows_removed']['invalid_dates'] = invalid_dates
            
            df = df[df['date'].notnull()]
            print(f"‚úì Removed rows. Remaining: {len(df):,}")
        
        print(f"\n‚úì Date parsing successful")
        print(f"  Date range: {df['date'].min().date()} to {df['date'].max().date()}")
        print(f"  Total days span: {(df['date'].max() - df['date'].min()).days}")
        
    except Exception as e:
        print(f"\n‚ùå Error parsing dates: {e}")
    
    # ========================================
    # STEP 8: REMOVE EXACT DUPLICATES
    # ========================================
    print(f"\n{'‚îÄ' * 120}")
    print("STEP 8: DUPLICATE DETECTION & REMOVAL")
    print(f"{'‚îÄ' * 120}")
    
    # Drop state_original before checking duplicates
    df_for_dedup = df.drop(columns=['state_original'], errors='ignore')
    
    before_dedup = len(df_for_dedup)
    exact_duplicates = df_for_dedup.duplicated().sum()
    
    print(f"\nüìä Exact duplicates: {exact_duplicates:,} ({exact_duplicates/before_dedup*100:.4f}%)")
    
    if exact_duplicates > 0:
        df = df_for_dedup.drop_duplicates()
        removed = before_dedup - len(df)
        quality_report['rows_removed']['exact_duplicates'] = removed
        print(f"‚úì Removed {removed:,} exact duplicates")
        print(f"‚úì Remaining rows: {len(df):,}")
    else:
        df = df_for_dedup
        print(f"‚úì No exact duplicates found")
    
    # ========================================
    # STEP 9: FEATURE ENGINEERING
    # ========================================
    print(f"\n{'‚îÄ' * 120}")
    print("STEP 9: FEATURE ENGINEERING")
    print(f"{'‚îÄ' * 120}")
    
    # Add calculated columns
    if dataset_name == "Enrolment":
        df['total_enrolments'] = df['age_0_5'] + df['age_5_17'] + df['age_18_greater']
        print(f"\n‚úì Added 'total_enrolments'")
        print(f"  Total: {df['total_enrolments'].sum():,}")
        
    elif dataset_name == "Biometric":
        df['total_bio_updates'] = df['bio_age_5_17'] + df['bio_age_17_']
        print(f"\n‚úì Added 'total_bio_updates'")
        print(f"  Total: {df['total_bio_updates'].sum():,}")
        
    elif dataset_name == "Demographic":
        df['total_demo_updates'] = df['demo_age_5_17'] + df['demo_age_17_']
        print(f"\n‚úì Added 'total_demo_updates'")
        print(f"  Total: {df['total_demo_updates'].sum():,}")
    
    # Add time features
    df['year'] = df['date'].dt.year
    df['month'] = df['date'].dt.month
    df['month_name'] = df['date'].dt.strftime('%B')
    df['quarter'] = df['date'].dt.quarter
    df['day_of_week'] = df['date'].dt.day_name()
    df['week_of_year'] = df['date'].dt.isocalendar().week
    df['day_of_month'] = df['date'].dt.day
    
    print(f"\n‚úì Added time features: year, month, month_name, quarter, day_of_week, week_of_year, day_of_month")
    
    # ========================================
    # FINAL SUMMARY
    # ========================================
    quality_report['final_rows'] = len(df)
    quality_report['total_removed'] = quality_report['initial_rows'] - quality_report['final_rows']
    quality_report['data_retained_percent'] = (quality_report['final_rows'] / quality_report['initial_rows']) * 100
    
    print(f"\n{'‚îÄ' * 120}")
    print(f"CLEANING SUMMARY FOR {dataset_name.upper()}")
    print(f"{'‚îÄ' * 120}")
    print(f"\n  Initial rows:      {quality_report['initial_rows']:>12,}")
    print(f"  Final rows:        {quality_report['final_rows']:>12,}")
    print(f"  Rows removed:      {quality_report['total_removed']:>12,}")
    print(f"  Data retained:     {quality_report['data_retained_percent']:>11.2f}%")
    print(f"\n  Unique states:     {df['state'].nunique():>12}")
    print(f"  Unique districts:  {df['district'].nunique():>12}")
    print(f"  Unique pincodes:   {df['pincode'].nunique():>12}")
    
    return df, quality_report

# ============================================
# PROCESS ALL DATASETS
# ============================================

all_quality_reports = {}

# Process Enrolment
df_enrol, qr_enrol = professional_data_cleaning(
    ENROLMENT_FOLDER, 
    "Enrolment",
    {'age_0_5': int, 'age_5_17': int, 'age_18_greater': int}
)
all_quality_reports['Enrolment'] = qr_enrol

# Process Biometric
df_bio, qr_bio = professional_data_cleaning(
    BIOMETRIC_FOLDER,
    "Biometric",
    {'bio_age_5_17': int, 'bio_age_17_': int}
)
all_quality_reports['Biometric'] = qr_bio

# Process Demographic
df_demo, qr_demo = professional_data_cleaning(
    DEMOGRAPHIC_FOLDER,
    "Demographic",
    {'demo_age_5_17': int, 'demo_age_17_': int}
)
all_quality_reports['Demographic'] = qr_demo

# ============================================
# SAVE CLEANED DATASETS
# ============================================
print(f"\n{'=' * 120}")
print("SAVING CLEANED DATASETS")
print(f"{'=' * 120}")

enrol_path = os.path.join(OUTPUT_FOLDER, "cleaned_enrolment.csv")
df_enrol.to_csv(enrol_path, index=False)
print(f"\n‚úì Saved: cleaned_enrolment.csv ({len(df_enrol):,} rows, {os.path.getsize(enrol_path)/(1024*1024):.2f} MB)")

bio_path = os.path.join(OUTPUT_FOLDER, "cleaned_biometric.csv")
df_bio.to_csv(bio_path, index=False)
print(f"‚úì Saved: cleaned_biometric.csv ({len(df_bio):,} rows, {os.path.getsize(bio_path)/(1024*1024):.2f} MB)")

demo_path = os.path.join(OUTPUT_FOLDER, "cleaned_demographic.csv")
df_demo.to_csv(demo_path, index=False)
print(f"‚úì Saved: cleaned_demographic.csv ({len(df_demo):,} rows, {os.path.getsize(demo_path)/(1024*1024):.2f} MB)")

# ============================================
# GENERATE QUALITY REPORT
# ============================================
print(f"\n{'=' * 120}")
print("GENERATING DATA QUALITY REPORT")
print(f"{'=' * 120}")

summary_data = {
    'Dataset': ['Enrolment', 'Biometric', 'Demographic', 'TOTAL'],
    'Initial_Rows': [
        qr_enrol['initial_rows'],
        qr_bio['initial_rows'],
        qr_demo['initial_rows'],
        qr_enrol['initial_rows'] + qr_bio['initial_rows'] + qr_demo['initial_rows']
    ],
    'Final_Rows': [
        qr_enrol['final_rows'],
        qr_bio['final_rows'],
        qr_demo['final_rows'],
        qr_enrol['final_rows'] + qr_bio['final_rows'] + qr_demo['final_rows']
    ],
    'Rows_Removed': [
        qr_enrol['total_removed'],
        qr_bio['total_removed'],
        qr_demo['total_removed'],
        qr_enrol['total_removed'] + qr_bio['total_removed'] + qr_demo['total_removed']
    ],
    'Data_Retained_%': [
        round(qr_enrol['data_retained_percent'], 2),
        round(qr_bio['data_retained_percent'], 2),
        round(qr_demo['data_retained_percent'], 2),
        round(((qr_enrol['final_rows'] + qr_bio['final_rows'] + qr_demo['final_rows']) / 
               (qr_enrol['initial_rows'] + qr_bio['initial_rows'] + qr_demo['initial_rows'])) * 100, 2)
    ],
    'Unique_States': [
        df_enrol['state'].nunique(),
        df_bio['state'].nunique(),
        df_demo['state'].nunique(),
        '-'
    ]
}

summary_df = pd.DataFrame(summary_data)
report_path = os.path.join(RESULTS_FOLDER, 'corrected_data_quality_report.csv')
summary_df.to_csv(report_path, index=False)

print(f"\n‚úì Quality Report:")
print(summary_df.to_string(index=False))
print(f"\n‚úì Saved: {report_path}")

# ============================================
# SAVE CORRECTION MAPPING
# ============================================
correction_summary = []
for original, corrected in STATE_NAME_CORRECTIONS.items():
    correction_summary.append({'Original_Name': original, 'Corrected_To': corrected})

correction_df = pd.DataFrame(correction_summary)
correction_path = os.path.join(RESULTS_FOLDER, 'state_name_corrections_applied.csv')
correction_df.to_csv(correction_path, index=False)
print(f"‚úì Saved correction mapping: {correction_path}")

# ============================================
# FINAL SUMMARY
# ============================================
print(f"\n{'=' * 120}")
print("‚úÖ CORRECTED PROFESSIONAL DATA CLEANING COMPLETE!")
print(f"{'=' * 120}")

print(f"\nüìä OVERALL STATISTICS:")
print(f"  Total rows processed:  {summary_data['Initial_Rows'][-1]:>12,}")
print(f"  Total rows cleaned:    {summary_data['Final_Rows'][-1]:>12,}")
print(f"  Total rows removed:    {summary_data['Rows_Removed'][-1]:>12,}")
print(f"  Overall data retained: {summary_data['Data_Retained_%'][-1]:>11.2f}%")

print(f"\nüîß CORRECTIONS APPLIED:")
print(f"  State name corrections: {len(STATE_NAME_CORRECTIONS)}")
print(f"  Historical names fixed: Orissa‚ÜíOdisha, Pondicherry‚ÜíPuducherry")
print(f"  Typos fixed: West Bangal‚ÜíWest Bengal, etc.")
print(f"  Format variations: & ‚Üí and, extra spaces removed")

print(f"\nüìÅ OUTPUT:")
print(f"  Cleaned data: {OUTPUT_FOLDER}")
print(f"  Quality reports: {RESULTS_FOLDER}")

print(f"\n‚úÖ ALL VALID INDIAN STATES/UTs PRESERVED!")
print(f"  No legitimate locations were removed")
print(f"  Only truly invalid entries (numbers, unknown names) removed")

print(f"\n{'=' * 120}")
print(f"Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"{'=' * 120}")
